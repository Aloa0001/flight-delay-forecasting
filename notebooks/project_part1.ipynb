{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9681342",
   "metadata": {},
   "source": [
    "## Project Part 1: Distributed Spark ML Pipeline\n\nThis notebook implements the mandatory steps for Project Part 1, focusing on distributed data handling, resource management, and training a machine learning model using Spark MLlib on a cluster with one master and two worker nodes.\n\n---\n\n### Step 1: Cluster Setup & Initial Spark Session\n\n**Requirement:** Set up a Spark cluster using Docker, VMware, or VirtualBox with 1 master node and 2 worker nodes. The setup must be configured for distributed task management and communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Note: Initial configuration for cluster connection and basic resources is set here.\n",
    "# We will further refine resource settings in Step 6.\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"FlightDelay-SparkSQL-Preprocessing\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"6\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark Session initialized on cluster master.\")\n",
    "print(f\"Spark UI (Monitor): http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2010ea",
   "metadata": {},
   "source": [
    "### Step 2 & 3: Manual Data Splitting & Loading on Each Node\n\n**Requirement:** The dataset is manually split across the master and worker nodes (`/data/part1.csv`, `/data/part2.csv`, `/data/part3.csv`). Each node loads its part, and Spark combines them into a unified DataFrame for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a46998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "print(f\"I am running on: {hostname}\")\n",
    "\n",
    "part_map = {\n",
    "    \"spark-master\":  \"/data/part1.csv\",\n",
    "    \"spark-worker1\": \"/data/part2.csv\",\n",
    "    \"spark-worker2\": \"/data/part3.csv\"\n",
    "}\n",
    "\n",
    "# 3. Data Loading on Each Node (Simulated/Demonstrated):\n",
    "# In a real distributed scenario, each executor/node would load its local file.\n",
    "my_file = part_map.get(hostname, \"/data/part1.csv\")\n",
    "df_my_part = spark.read.csv(my_file, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"I only loaded: {my_file}\")\n",
    "print(f\"Rows in my part: {df_my_part.count():,}\")\n",
    "\n",
    "# The simplest way to load the unified dataset across all nodes' partitions is via wildcards (Spark handles distribution).\n",
    "# This simulates the unified DataFrame Spark works with after loading individual pieces.\n",
    "df_full = spark.read.csv(\"/data/part*.csv\", header=True, inferSchema=True).withColumnRenamed(\"ArrDelay\", \"label\")\n",
    "df_full.createOrReplaceTempView(\"flights_raw\")\n",
    "print(f\"\\nFull unified dataset (Raw): {df_full.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf472c36",
   "metadata": {},
   "source": [
    "### Step 4: Data Partitioning and Processing with Spark SQL\n\n**Requirement:** Repartition the dataset for parallel processing, perform initial data cleaning, exploration, and feature engineering using SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a. Cleaning, Transformation, and Repartitioning (via DataFrame API for caching)\n",
    "# The `.repartition(6)` step distributes the data across the cluster for parallel processing.\n",
    "clean_df = spark.table(\"flights_raw\") \\\n",
    "    .where(\"Cancelled = 0 AND Diverted = 0 AND label IS NOT NULL\") \\\n",
    "    .selectExpr(\n",
    "        \"CAST(label AS DOUBLE) AS label\",\n",
    "        \"CAST(DepDelay AS DOUBLE) AS DepDelay\",\n",
    "        \"CAST(Distance AS DOUBLE) AS Distance\",\n",
    "        \"CAST(TaxiIn AS DOUBLE) AS TaxiIn\",\n",
    "        \"CAST(TaxiOut AS DOUBLE) AS TaxiOut\",\n",
    "        \"UniqueCarrier\",\n",
    "        \"Origin\",\n",
    "        \"Dest\",\n",
    "        \"DayOfWeek\"\n",
    ") \\\n",
    "    .repartition(6) \\\n",
    "    .cache()\n",
    "\n",
    "print(f\"Dataset repartitioned into {clean_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Rows after filtering: {clean_df.count():,}\")\n",
    "\n",
    "# 4b. Feature Engineering and Exploration using Spark SQL\n",
    "clean_df.createOrReplaceTempView(\"flights_sql\")\n",
    "\n",
    "# Add feature engineering using SQL\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW flights_features AS\n",
    "SELECT \n",
    "    *,\n",
    "    -- Feature engineering with SQL\n",
    "    CASE WHEN DayOfWeek IN (6,7) THEN 1.0 ELSE 0.0 END AS IsWeekend,\n",
    "    Distance * 1.60934 AS Distance_km,\n",
    "    CASE WHEN DepDelay > 15 THEN 1.0 ELSE 0.0 END AS WasDepartureDelayed\n",
    "FROM flights_sql\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- Data Exploration ---\")\n",
    "spark.sql(\"SELECT COUNT(*) AS total_flights FROM flights_features\").show(1)\n",
    "spark.sql(\"SELECT AVG(label) AS avg_delay_min FROM flights_features\").show(1)\n",
    "spark.sql(\"SELECT UniqueCarrier, AVG(label) AS avg_delay, COUNT(*) AS flights FROM flights_features GROUP BY UniqueCarrier ORDER BY avg_delay DESC LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43c6cc",
   "metadata": {},
   "source": [
    "### Step 5: Machine Learning Pipeline (MLlib)\n\n**Requirement:** Implement a machine learning pipeline using Spark MLlib, including feature transformations, model training, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import time\n",
    "\n",
    "# --- 5a. Data Preparation and Preprocessing Pipeline ---\n",
    "data = spark.table(\"flights_features\").cache()\n",
    "train_df, test_df = data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set: {train_df.count():,} rows\")\n",
    "print(f\"Test set: {test_df.count():,} rows\")\n",
    "\n",
    "# Reduce cardinality for categorical features to keep the Decision Tree/Random Forest models performant\n",
    "# This step replaces less frequent values with 'Other' (required to be outside the MLlib Pipeline)\n",
    "top_origins = [r[0] for r in train_df.groupBy(\"Origin\").count().orderBy(\"count\", ascending=False).limit(30).select(\"Origin\").collect()]\n",
    "top_dests = [r[0] for r in train_df.groupBy(\"Dest\").count().orderBy(\"count\", ascending=False).limit(30).select(\"Dest\").collect()]\n",
    "\n",
    "def apply_grouping(df):\n",
    "    df = df.withColumn(\"Origin_group\", when(col(\"Origin\").isin(top_origins), col(\"Origin\")).otherwise(\"Other\"))\n",
    "    df = df.withColumn(\"Dest_group\",   when(col(\"Dest\").isin(top_dests),   col(\"Dest\")).otherwise(\"Other\"))\n",
    "    return df\n",
    "\n",
    "train_grouped = apply_grouping(train_df)\n",
    "test_grouped = apply_grouping(test_df)\n",
    "\n",
    "# Define MLlib preprocessing stages\n",
    "indexer_carrier = StringIndexer(inputCol=\"UniqueCarrier\", outputCol=\"carrier_idx\", handleInvalid=\"keep\")\n",
    "indexer_origin = StringIndexer(inputCol=\"Origin_group\", outputCol=\"origin_idx\", handleInvalid=\"keep\")\n",
    "indexer_dest = StringIndexer(inputCol=\"Dest_group\", outputCol=\"dest_idx\", handleInvalid=\"keep\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"DepDelay\", \"Distance_km\", \"TaxiIn\", \"TaxiOut\", \"IsWeekend\", \n",
    "               \"carrier_idx\", \"origin_idx\", \"dest_idx\"], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Define models (estimators)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20)\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", numTrees=50, maxDepth=10, seed=42)\n",
    "\n",
    "# Build two full pipelines: Preprocessing + Model\n",
    "lr_pipeline = Pipeline(stages=[indexer_carrier, indexer_origin, indexer_dest, assembler, lr])\n",
    "rf_pipeline = Pipeline(stages=[indexer_carrier, indexer_origin, indexer_dest, assembler, rf])\n",
    "\n",
    "print(\"\\nMLlib pipelines defined and ready for fitting (training).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a891055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5b. Model Training & Regression Evaluation ---\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", metricName=\"r2\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"label\", metricName=\"mae\")\n",
    "\n",
    "print(\"\\n--- Training Linear Regression ---\")\n",
    "start_lr = time.time()\n",
    "lr_model = lr_pipeline.fit(train_grouped)\n",
    "lr_time = time.time() - start_lr\n",
    "lr_pred = lr_model.transform(test_grouped)\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_pred)\n",
    "lr_r2 = evaluator_r2.evaluate(lr_pred)\n",
    "lr_mae = evaluator_mae.evaluate(lr_pred)\n",
    "lr_pred.cache()\n",
    "\n",
    "print(\"\\n--- Training Random Forest ---\")\n",
    "start_rf = time.time()\n",
    "rf_model = rf_pipeline.fit(train_grouped)\n",
    "rf_time = time.time() - start_rf\n",
    "rf_pred = rf_model.transform(test_grouped)\n",
    "rf_rmse = evaluator_rmse.evaluate(rf_pred)\n",
    "rf_r2 = evaluator_r2.evaluate(rf_pred)\n",
    "rf_mae = evaluator_mae.evaluate(rf_pred)\n",
    "rf_pred.cache()\n",
    "\n",
    "print(\"\\n--- Regression Results ---\")\n",
    "print(f\"Linear Regression → RMSE: {lr_rmse:.2f} min | R²: {lr_r2:.4f} | MAE: {lr_mae:.2f} min | Time: {lr_time:.1f}s\")\n",
    "print(f\"Random Forest → RMSE: {rf_rmse:.2f} min | R²: {rf_r2:.4f} | MAE: {rf_mae:.2f} min | Time: {rf_time:.1f}s\")\n",
    "\n",
    "# Save models for later use\n",
    "lr_model.save(\"/workspace/models/lr_model_final\")\n",
    "rf_model.save(\"/workspace/models/rf_model_final\")\n",
    "print(\"\\nModels saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3052608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5c. Classification Evaluation (Optional but helpful) ---\n",
    "\n",
    "# Convert to binary classification: delayed >= 15 min (used in the project requirements markdown)\n",
    "lr_pred_class = lr_pred.withColumn(\"delayed_15\", (col(\"label\") >= 15).cast(\"double\")) \\\n",
    "    .withColumn(\"pred_delayed_15\", (col(\"prediction\") >= 15).cast(\"double\"))\n",
    "\n",
    "rf_pred_class = rf_pred.withColumn(\"delayed_15\", (col(\"label\") >= 15).cast(\"double\")) \\\n",
    "    .withColumn(\"pred_delayed_15\", (col(\"prediction\") >= 15).cast(\"double\"))\n",
    "\n",
    "# Evaluators\n",
    "bin_eval = BinaryClassificationEvaluator(labelCol=\"delayed_15\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "multi_eval = MulticlassClassificationEvaluator(labelCol=\"delayed_15\", predictionCol=\"pred_delayed_15\")\n",
    "\n",
    "lr_auc = bin_eval.evaluate(lr_pred_class)\n",
    "rf_auc = bin_eval.evaluate(rf_pred_class)\n",
    "lr_acc = multi_eval.evaluate(lr_pred_class, {multi_eval.metricName: \"accuracy\"})\n",
    "rf_acc = multi_eval.evaluate(rf_pred_class, {multi_eval.metricName: \"accuracy\"})\n",
    "lr_f1 = multi_eval.evaluate(lr_pred_class, {multi_eval.metricName: \"f1\"})\n",
    "rf_f1 = multi_eval.evaluate(rf_pred_class, {multi_eval.metricName: \"f1\"})\n",
    "\n",
    "# Print classification results\n",
    "print(\"\\n--- Classification Results (Delayed ≥ 15 min?) ---\")\n",
    "print(f\"Linear Regression → AUC: {lr_auc:.4f} | Accuracy: {lr_acc:.4f} | F1-score: {lr_f1:.4f}\")\n",
    "print(f\"Random Forest → AUC: {rf_auc:.4f} | Accuracy: {rf_acc:.4f} | F1-score: {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0c0d1",
   "metadata": {},
   "source": [
    "### Step 6: Resource Management\n\n**Requirement:** Configure Spark resource settings (`spark.executor.memory`, `spark.executor.cores`) and monitor utilization. Fine-tune `spark.sql.shuffle.partitions`.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e90c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SPARK CLUSTER & RESOURCE CONFIGURATION ===\")\n",
    "\n",
    "# 6.1 & 6.2: Configuring Memory and Cores (Master-side check)\n",
    "# These settings are primarily controlled by the Docker/VM setup (e.g., spark-defaults.conf or environment variables).\n",
    "# We print the current values for documentation.\n",
    "\n",
    "print(f\"Shuffle partitions (Initial/Driver Config) : {spark.conf.get('spark.sql.shuffle.partitions', 'Not set')}\")\n",
    "print(f\"Executor memory (Initial/Driver Config)    : {spark.conf.get('spark.executor.memory', 'Not set')}\")\n",
    "\n",
    "try:\n",
    "    # Note: spark.executor.cores is often a fixed setting per worker set in spark-defaults.conf\n",
    "    print(f\"Executor cores (Config)                    : {spark.conf.get('spark.executor.cores', 'Not set')}\")\n",
    "    print(f\"Default parallelism (from executors)     : {spark.sparkContext.defaultParallelism}\")\n",
    "except Exception:\n",
    "    print(\"Executor cores/Parallelism not directly readable via spark.conf.get for static setup.\")\n",
    "\n",
    "print(f\"Dynamic allocation enabled (Check)           : {spark.conf.get('spark.dynamicAllocation.enabled', 'false')}\")\n",
    "\n",
    "print(\"\\n--- Monitoring (Step 6.3 & 7) ---\")\n",
    "print(\"Resource Utilization is monitored using the Spark Web UI (http://localhost:4040).\"+\n    \"This includes Task Distribution, Memory, and CPU usage on the Executors tab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16d123",
   "metadata": {},
   "source": [
    "### Step 6.4 & 7: Fine-Tuning Shuffle Partitions & Performance Monitoring\n\n**Requirement:** Adjust shuffle partitions to balance parallelism and memory efficiency, and monitor performance using a micro-benchmark (Step 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f137d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "# Use the clean_df created in Step 4 which is cached\n",
    "df_to_shuffle = spark.table(\"flights_features\")\n",
    "\n",
    "def time_shuffle_job(df, partitions):\n",
    "    spark.conf.set('spark.sql.shuffle.partitions', str(partitions))\n",
    "    # Clear cache to ensure a fresh run if same DF is used (optional, but safer)\n",
    "    df.unpersist(blocking=True)\n",
    "    t0 = perf_counter()\n",
    "    # Lightweight aggregation that triggers shuffle and forces re-evaluation\n",
    "    _ = df.groupBy('UniqueCarrier').count().count()\n",
    "    t1 = perf_counter()\n",
    "    return t1 - t0\n",
    "\n",
    "\n",
    "print(\"\\n--- Fine-Tuning spark.sql.shuffle.partitions ---\")\n",
    "\n",
    "# Number of partitions to test\n",
    "partitions_to_test = [6, 12, 24]\n",
    "results = {}\n",
    "\n",
    "for p in partitions_to_test:\n",
    "    t = time_shuffle_job(df_to_shuffle, p)\n",
    "    results[p] = t\n",
    "    print(f\"Shuffle partitions = {p:2d} → Execution time: {t:.3f} seconds\")\n",
    "\n",
    "best_p = min(results, key=results.get)\n",
    "print(f\"\\nObservation: Shuffle partitions = {best_p} is the most efficient configuration for this job.\")\n",
    "spark.conf.set('spark.sql.shuffle.partitions', str(best_p)) # Apply best setting\n",
    "print(f\"Final Shuffle Partitions set to: {best_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e1d52",
   "metadata": {},
   "source": [
    "### Step 8: Tuning the Machine Learning Model (Cross-Validation)\n\n**Requirement:** Fine-tune the machine learning model by adjusting hyperparameters and running cross-validation to find the best-performing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647429d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "# Re-create the full LR pipeline for tuning, as the original LR object is inside the fitted lr_model\n",
    "# We will use the same preprocessing stages as defined in Step 5 (indexer_carrier, indexer_origin, indexer_dest, assembler)\n",
    "lr_to_tune = LinearRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=30)\n",
    "lr_pipeline_tune = Pipeline(stages=lr_pipeline.getStages()[:-1] + [lr_to_tune])\n",
    "\n",
    "# Define the hyperparameter grid for Linear Regression\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr_to_tune.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr_to_tune.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=lr_pipeline_tune, estimatorParamMaps=paramGrid,\n",
    "                    evaluator=RegressionEvaluator(labelCol=\"label\"),\n",
    "                    numFolds=3, parallelism=4, seed=42)\n",
    "\n",
    "print(\"Running 3-fold Cross-Validation on Linear Regression to tune: regParam and elasticNetParam...\")\n",
    "cvModel = cv.fit(train_grouped)\n",
    "\n",
    "best_rmse = RegressionEvaluator().evaluate(cvModel.transform(test_grouped))\n",
    "print(f\"\\nBest model after Cross-Validation → RMSE: {best_rmse:.2f} minutes\")\n",
    "print(\"Best parameters found:\", cvModel.bestModel.extractParamMap())\n",
    "\n",
    "# Save the best tuned model\n",
    "cvModel.bestModel.save(\"/workspace/models/lr_best_tuned\")\n",
    "print(\"Best tuned model saved to /workspace/models/lr_best_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9681342-final",
   "metadata": {},
   "source": [
    "## Summary of Part 1 & Next Steps\n\nThis notebook has successfully completed all mandatory steps for Part 1:\n\n* **Cluster Setup & Data Loading (Steps 1-3):** A Spark session was established, and the distributed loading of the manually split dataset was demonstrated.\n* **Data Processing (Step 4):** Cleaning, feature engineering (like `IsWeekend`, `Distance_km`, `WasDepartureDelayed`), and repartitioning were executed using Spark SQL/DataFrame API.\n* **ML Pipeline & Evaluation (Step 5):** Two distributed machine learning pipelines (`LinearRegression` and `RandomForestRegressor`) were trained and evaluated for regression and classification metrics.\n* **Resource Management & Tuning (Steps 6-8):** The current resource configuration was checked, shuffle partitions were micro-tuned, and the Linear Regression model was fine-tuned using Cross-Validation.\n\nAll key outcomes and required steps have been documented and implemented in the runnable cells. You can now use the results from the executed cells to draft the **Methodology** and **Results** sections of your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session after all work is complete\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}