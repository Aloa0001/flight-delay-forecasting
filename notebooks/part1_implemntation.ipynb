{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212e0633",
   "metadata": {},
   "source": [
    "# Step 1: Cluster Setup & Resource Management\n",
    "\n",
    "**Requirement:** - **Cluster Setup (Step 1):** Connect to the distributed cluster (1 Master, 2 Workers).\n",
    "\n",
    "- **Resource Management (Step 4, 6):** Explicitly configure `spark.executor.memory`, `spark.executor.cores`, and `spark.sql.shuffle.partitions` to optimize performance and prevent resource contention.\n",
    "\n",
    "**Configuration Strategy:**\n",
    "\n",
    "- **Executor Memory:** Set to `1g` (leaving buffer for OS/Overhead within the 2GB container limit).\n",
    "- **Executor Cores:** Set to `2` (utilizing the available cores per worker).\n",
    "- **Shuffle Partitions:** Set to `6` (Calculation: 2 workers Ã— 3 partitions/worker) to ensure balanced parallelism during joins/aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ba960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/16 11:19:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import socket\n",
    "import time\n",
    "\n",
    "# initialize spark session with clear resource management config\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"FlightDelay-Mandatory-Part1\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    # --- Resource Management (Req 6) ---\n",
    "    .config(\"spark.executor.memory\", \"1g\")  # memory per worker\n",
    "    .config(\"spark.executor.cores\", \"2\")  # cores per worker\n",
    "    .config(\"spark.cores.max\", \"4\")  # total cores in cluster\n",
    "    # balanced partitions for shuffling\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"6\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98f904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SPARK CLUSTER CONNECTION\n",
      "==================================================\n",
      "Version    : 3.5.0\n",
      "Master URL       : spark://spark-master:7077\n",
      "App Name         : FlightDelay-Mandatory-Part1\n",
      "Running on Node  : spark-master\n",
      "--------------------------------------------------\n",
      "Executor Memory  : 1g\n",
      "Executor Cores   : 2\n",
      "Partitions: 6\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# verify conn\n",
    "print(\"=\" * 50)\n",
    "print(\"SPARK CLUSTER CONNECTION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Version    : {spark.version}\")\n",
    "print(f\"Master URL       : {spark.sparkContext.master}\")\n",
    "print(f\"App Name         : {spark.sparkContext.appName}\")\n",
    "print(f\"Running on Node  : {socket.gethostname()}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Executor Memory  : {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"Executor Cores   : {spark.conf.get('spark.executor.cores')}\")\n",
    "print(f\"Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cd834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster is active. Check Spark UI at http://localhost:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# trigger the UI registration\n",
    "spark.range(5).collect()\n",
    "print(\"Cluster is active. Check Spark UI at http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c8a82",
   "metadata": {},
   "source": [
    "# Step 2: Manual Data Splitting and Node Assignment\n",
    "\n",
    "**Requirement:**\n",
    "\n",
    "- **Manual Splitting (Step 2):** The dataset must be manually split into parts.\n",
    "- **Node Assignment:** Worker 1, Worker 2, and the Master must each hold a specific portion of the data.\n",
    "\n",
    "**Implementation:**\n",
    "Verify the existence of the three split files (`part1.csv`, `part2.csv`, `part3.csv`) in the shared volume -> define a logical mapping (`node_file_map`) that restricts each node to access _only_ its assigned file during the independent processing phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each node \"receives one portition\"\n",
    "node_file_map = {\n",
    "    \"spark-master\": \"/data/part1.csv\",\n",
    "    \"spark-worker1\": \"/data/part2.csv\",\n",
    "    \"spark-worker2\": \"/data/part3.csv\",\n",
    "}\n",
    "\n",
    "total_rows = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: spark-master    | File: /data/part1.csv | Rows: 201,921  | Size: 21.57 MB\n",
      "Node: spark-worker1   | File: /data/part2.csv | Rows: 201,921  | Size: 21.55 MB\n",
      "Node: spark-worker2   | File: /data/part3.csv | Rows: 201,923  | Size: 21.56 MB\n",
      "__________________________________________________\n",
      "Total Dataset Size: 1,211,530 rows\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# data split verification: files exist and count rows\n",
    "for node, file_path in node_file_map.items():\n",
    "    if os.path.exists(file_path):\n",
    "        # row count (subtract 1 for header)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            row_count = sum(1 for line in f) - 1\n",
    "\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(\n",
    "            f\"Node: {node:<15} | File: {file_path:<15} | Rows: {row_count:<8,} | Size: {file_size_mb:.2f} MB\"\n",
    "        )\n",
    "        total_rows += row_count\n",
    "    else:\n",
    "        print(f\"ERROR: {file_path} not found for {node}!\")\n",
    "\n",
    "print(\"_\" * 50)\n",
    "print(f\"Total Dataset Size: {total_rows:,} rows\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d5ee9",
   "metadata": {},
   "source": [
    "# Step 3: Distributed Loading, Preprocessing & Optimization\n",
    "\n",
    "**Requirement:**\n",
    "\n",
    "- **Data Loading (Step 3):** Load data from all nodes and combine into a unified DataFrame.\n",
    "- **Spark SQL Processing (Step 4):** Use SQL queries for cleaning and feature engineering.\n",
    "- **Resource Management (Step 6):** Adjust shuffle partitions and cache data for performance.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "1.  **Load:** `/data/part*.csv`. Spark's driver coordinates the workers to read their local file parts in parallel.\n",
    "2.  **Clean:** register a temp view `flights_raw` and use Spark SQL to cast columns and filter out cancelled flights.\n",
    "3.  **Optimize:** repartition the dataframe to `6` (matching the shuffle configuration) and call `.cache()` to store it in the workers' RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06719ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DISTRIBUTED PREPROCESSING\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 12:04:26 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:=======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Input Partitions      : 4\n",
      "Output Partitions     : 6\n",
      "Final Row Count       : 587,130\n",
      "Duration           : 9.97 seconds\n",
      "Is Cached?            : True\n",
      "==================================================\n",
      "root\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DISTRIBUTED PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# distributed Loading (reaad part1, part2, part3 in parallel)\n",
    "df_raw = spark.read.csv(\"/data/part*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Spark SQL Preprocessing\n",
    "df_raw.createOrReplaceTempView(\"flights_raw\")\n",
    "\n",
    "# performing casting and filtering (Requirement: \"Data Partitioning and Processing\")\n",
    "df_clean = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        -- target\n",
    "        CAST(ArrDelay AS DOUBLE) AS ArrDelay,\n",
    "        \n",
    "        -- numeric features \n",
    "        CAST(DepDelay AS DOUBLE) AS DepDelay,\n",
    "        CAST(Distance AS DOUBLE) AS Distance,\n",
    "        CAST(TaxiOut AS DOUBLE) AS TaxiOut,\n",
    "        CAST(CRSElapsedTime AS DOUBLE) AS CRSElapsedTime,\n",
    "        \n",
    "        -- categorical features\n",
    "        DayOfWeek,\n",
    "        UniqueCarrier,\n",
    "        Origin,\n",
    "        Dest\n",
    "        \n",
    "    FROM flights_raw\n",
    "    WHERE Cancelled = 0 \n",
    "      AND Diverted = 0\n",
    "      AND ArrDelay IS NOT NULL\n",
    "      AND DepDelay IS NOT NULL\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# repartition & Cache (Requirement: \"Resource Management\")\n",
    "# repartition to 6 to match our shuffle config\n",
    "df_processed = df_clean.repartition(6).cache()\n",
    "\n",
    "# trigger action to materialize cache and count rows\n",
    "row_count = df_processed.count()\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(\"_\" * 50)\n",
    "print(f\"Input Partitions      : {df_raw.rdd.getNumPartitions()}\")\n",
    "print(f\"Output Partitions     : {df_processed.rdd.getNumPartitions()}\")\n",
    "print(f\"Final Row Count       : {row_count:,}\")\n",
    "print(f\"Duration           : {duration:.2f} seconds\")\n",
    "print(f\"Is Cached?            : {df_processed.is_cached}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_processed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43965423",
   "metadata": {},
   "source": [
    "# Step 4: Distributed ML Pipeline with Classification Evaluation\n",
    "\n",
    "**Requirement:**\n",
    "\n",
    "- **Models (Step 5):** Implement Machine Learning models (Linear Regression and Random Forest).\n",
    "- **Evaluation (Step 5):** Evaluate performance using **Accuracy, Precision, and F1-score**.\n",
    "\n",
    "**Implementation Strategy:**\n",
    "\n",
    "1.  **Feature Engineering:**\n",
    "\n",
    "    - Convert `Origin`/`Dest` (high cardinality) and `UniqueCarrier` into numerical vectors using `StringIndexer` and `OneHotEncoder`.\n",
    "    - Assemble with numeric features (`DepDelay`, `Distance`, etc.).\n",
    "\n",
    "2.  **Distributed Training:** Train two regression models to predict the exact delay in minutes.\n",
    "\n",
    "3.  **Metric Calculation (The Threshold Strategy):**\n",
    "    - Since the models predict _time_ (continuous), but the requirements ask for _classification metrics_ (Accuracy/F1) -> apply the **15-minute threshold**.\n",
    "    - **Logic:** If `Predicted_Delay > 15.0` $\\rightarrow$ Class 1 (Late), else Class 0 (On Time).\n",
    "      - then compute Accuracy, Weighted Precision, and Weighted F1-Score based on this binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579b39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Rows: 469,668 | Test Rows: 117,462\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# handle high-cardinality categoricals (optimization)\n",
    "# keep top 20 airports, others become \"other\" to prevent massive feature vectors\n",
    "top_origins = [\n",
    "    r[0]\n",
    "    for r in df_processed.groupBy(\"Origin\")\n",
    "    .count()\n",
    "    .orderBy(col(\"count\").desc())\n",
    "    .limit(20)\n",
    "    .collect()\n",
    "]\n",
    "top_dests = [\n",
    "    r[0]\n",
    "    for r in df_processed.groupBy(\"Dest\")\n",
    "    .count()\n",
    "    .orderBy(col(\"count\").desc())\n",
    "    .limit(20)\n",
    "    .collect()\n",
    "]\n",
    "\n",
    "df_ml = df_processed.withColumn(\n",
    "    \"Origin_group\",\n",
    "    when(col(\"Origin\").isin(top_origins), col(\"Origin\")).otherwise(\"Other\"),\n",
    ").withColumn(\n",
    "    \"Dest_group\", when(col(\"Dest\").isin(top_dests), col(\"Dest\")).otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# split data\n",
    "train_data, test_data = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train Rows: {train_data.count():,} | Test Rows: {test_data.count():,}\")\n",
    "\n",
    "# pipeline stages\n",
    "stages = []\n",
    "categorical_cols = [\"UniqueCarrier\", \"Origin_group\", \"Dest_group\"]\n",
    "numeric_cols = [\"DepDelay\", \"Distance\", \"TaxiOut\", \"CRSElapsedTime\", \"DayOfWeek\"]\n",
    "\n",
    "for c in categorical_cols:\n",
    "    stages.append(StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\"))\n",
    "    stages.append(OneHotEncoder(inputCol=c + \"_idx\", outputCol=c + \"_vec\"))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_cols + [c + \"_vec\" for c in categorical_cols],\n",
    "    outputCol=\"features\",\n",
    ")\n",
    "stages.append(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3349f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 13:49:33 WARN Instrumentation: [fc1458d5] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/12/16 13:49:35 WARN Instrumentation: [fc1458d5] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Accuracy: 0.9233 | F1: 0.9218 | Time: 6.44s\n",
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:======================================>                  (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Accuracy: 0.9070 | F1: 0.9049 | Time: 14.07s\n",
      "\n",
      "============================================================\n",
      "Model                | Accuracy   | Precision  | F1-Score   | Time (s)  \n",
      "---------------------------------------------------------------------------\n",
      "Linear Regression    | 0.9233     | 0.9221     | 0.9218     | 6.44      \n",
      "Random Forest        | 0.9070     | 0.9051     | 0.9049     | 14.07     \n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train & evaluate models\n",
    "models = [\n",
    "    (\n",
    "        \"Linear Regression\",\n",
    "        LinearRegression(labelCol=\"ArrDelay\", featuresCol=\"features\", maxIter=10),\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        RandomForestRegressor(\n",
    "            labelCol=\"ArrDelay\", featuresCol=\"features\", numTrees=20, maxDepth=5\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# train & evaluate loop\n",
    "for name, algo in models:\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    pipeline = Pipeline(stages=stages + [algo])\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = pipeline.fit(train_data)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # make predictions\n",
    "    raw_preds = model.transform(test_data)\n",
    "\n",
    "    # CONVERT TO BINARY CLASS FOR METRICS\n",
    "    # threshold: 15 minutes\n",
    "    preds_binary = raw_preds.withColumn(\n",
    "        \"label_class\", when(col(\"ArrDelay\") >= 15, 1.0).otherwise(0.0)\n",
    "    ).withColumn(\"pred_class\", when(col(\"prediction\") >= 15, 1.0).otherwise(0.0))\n",
    "\n",
    "    # calculate classification metrics\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label_class\", predictionCol=\"pred_class\", metricName=\"accuracy\"\n",
    "    )\n",
    "    evaluator_prec = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label_class\",\n",
    "        predictionCol=\"pred_class\",\n",
    "        metricName=\"weightedPrecision\",\n",
    "    )\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label_class\", predictionCol=\"pred_class\", metricName=\"f1\"\n",
    "    )\n",
    "\n",
    "    acc = evaluator_acc.evaluate(preds_binary)\n",
    "    prec = evaluator_prec.evaluate(preds_binary)\n",
    "    f1 = evaluator_f1.evaluate(preds_binary)\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"Precision\": prec, \"F1\": f1, \"Time\": train_time}\n",
    "    print(f\"Done. Accuracy: {acc:.4f} | F1: {f1:.4f} | Time: {train_time:.2f}s\")\n",
    "\n",
    "# comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\n",
    "    f\"{'Model':<20} | {'Accuracy':<10} | {'Precision':<10} | {'F1-Score':<10} | {'Time (s)':<10}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "for name, m in results.items():\n",
    "    print(\n",
    "        f\"{name:<20} | {m['Accuracy']:<10.4f} | {m['Precision']:<10.4f} | {m['F1']:<10.4f} | {m['Time']:<10.2f}\"\n",
    "    )\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf416cd6",
   "metadata": {},
   "source": [
    "# Step 5: Model Tuning (Hyperparameter Optimization)\n",
    "\n",
    "**Requirement:**\n",
    "\n",
    "- **Tuning (Page 4):** Adjust hyperparameters (e.g., regularization) and run Cross-Validation to find the best configuration.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "- use `CrossValidator` with a `ParamGridBuilder`.\n",
    "- tune the **Linear Regression** model (since it was our best performer).\n",
    "- **Hyperparameters to tune:**\n",
    "  - `regParam`: Regularization parameter (0.01 vs 0.1).\n",
    "  - `elasticNetParam`: Mixing parameter (0.0 for L2, 0.5 for ElasticNet).\n",
    "- **Folds:** 3-Fold Cross-Validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# define the estimator (pipeline with LR)\n",
    "# - reuse the pipeline stages from Step 4, but with a new LR instance\n",
    "lr_tune = LinearRegression(labelCol=\"ArrDelay\", featuresCol=\"features\", maxIter=10)\n",
    "pipeline_tune = Pipeline(stages=stages + [lr_tune])\n",
    "\n",
    "# define the ParamGrid\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr_tune.regParam, [0.01, 0.1])\n",
    "    .addGrid(lr_tune.elasticNetParam, [0.0, 0.5])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# define CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline_tune,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"rmse\"),\n",
    "    numFolds=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search Complete in 40.84s\n",
      "------------------------------------------------------------\n",
      "Best RegParam: 0.01\n",
      "Best ElasticNetParam: 0.0\n",
      "Test RMSE (Tuned Model): 10.34\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Cross-Validation...\")\n",
    "start_cv = time.time()\n",
    "\n",
    "# run tunning\n",
    "cvModel = crossval.fit(train_data)\n",
    "end_cv = time.time()\n",
    "\n",
    "# get best model results\n",
    "best_model = cvModel.bestModel.stages[-1]\n",
    "print(f\"\\nGrid Search Complete in {end_cv - start_cv:.2f}s\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Best RegParam: {best_model.getRegParam()}\")\n",
    "print(f\"Best ElasticNetParam: {best_model.getElasticNetParam()}\")\n",
    "\n",
    "# evaluate best model on test data\n",
    "preds_tuned = cvModel.transform(test_data)\n",
    "rmse_tuned = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"rmse\").evaluate(\n",
    "    preds_tuned\n",
    ")\n",
    "print(f\"Test RMSE (Tuned Model): {rmse_tuned:.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64326d",
   "metadata": {},
   "source": [
    "# Part 1 - Part 2: Comparative Analysis (Independent vs. Distributed)\n",
    "\n",
    "**Objective:**\n",
    "Compare two execution paradigms as per Project Instructions Part 2:\n",
    "\n",
    "1.  **Scenario A (Independent):** Simultaneous ML jobs running on isolated data chunks (simulating separate worker tasks).\n",
    "2.  **Scenario B (Distributed):** One distributed ML job running on the unified dataset.\n",
    "\n",
    "**Methodology:**\n",
    "\n",
    "- use Python `threading` to launch 3 concurrent Spark jobs.\n",
    "- each job loads a specific partition (`part1.csv`, `part2.csv`, `part3.csv`) and trains a local Linear Regression model.\n",
    "- compare the **Average Accuracy** and **Total Time** against the Distributed Model results from Step 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894fd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# dictionary to store results from the 3 concurrent jobs\n",
    "independent_results = {}\n",
    "\n",
    "\n",
    "def train_local_simulation(node_name, data_path):\n",
    "    \"\"\"\n",
    "    simulates a worker node training a model on its isolated data partition.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[{node_name}] Starting local training on {data_path}...\")\n",
    "        start_local = time.time()\n",
    "\n",
    "        # load ONLY the specific local file\n",
    "        # - simulate isolation by reading specific paths.\n",
    "        df_chunk = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "        # local preprocessing (\n",
    "        # - replicate the main cleaning logic (filter cancellations, cast types)\n",
    "        df_clean = (\n",
    "            df_chunk.filter(\n",
    "                \"Cancelled == 0 AND Diverted == 0 AND ArrDelay IS NOT NULL AND DepDelay IS NOT NULL\"\n",
    "            )\n",
    "            .withColumn(\"ArrDelay\", col(\"ArrDelay\").cast(\"double\"))\n",
    "            .withColumn(\"DepDelay\", col(\"DepDelay\").cast(\"double\"))\n",
    "            .withColumn(\"Distance\", col(\"Distance\").cast(\"double\"))\n",
    "        )\n",
    "\n",
    "        # feature engineering\n",
    "        # - treat Origin/Dest as strings directly\n",
    "        # - skip high-cardinality for speed in this test\n",
    "        # use numeric features which are consistent across partitions\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[\"DepDelay\", \"Distance\"], outputCol=\"features\"\n",
    "        )\n",
    "\n",
    "        # local model training\n",
    "        lr = LinearRegression(labelCol=\"ArrDelay\", featuresCol=\"features\", maxIter=10)\n",
    "        pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "        # split 80/20 locally\n",
    "        train, test = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "        model = pipeline.fit(train)\n",
    "\n",
    "        # local model eval\n",
    "        preds = model.transform(test)\n",
    "\n",
    "        # classif metrics (threshold 15m)\n",
    "        preds_bin = preds.withColumn(\n",
    "            \"label_class\", when(col(\"ArrDelay\") >= 15, 1.0).otherwise(0.0)\n",
    "        ).withColumn(\"pred_class\", when(col(\"prediction\") >= 15, 1.0).otherwise(0.0))\n",
    "\n",
    "        acc = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"label_class\", predictionCol=\"pred_class\", metricName=\"accuracy\"\n",
    "        ).evaluate(preds_bin)\n",
    "\n",
    "        duration = time.time() - start_local\n",
    "        independent_results[node_name] = {\"Accuracy\": acc, \"Time\": duration}\n",
    "        print(f\"[{node_name}] Finished. Acc: {acc:.4f} | Time: {duration:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{node_name}] FAILED: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Job_Master] Starting local training on /data/part1.csv...\n",
      "[Job_Worker1] Starting local training on /data/part2.csv...\n",
      "[Job_Worker2] Starting local training on /data/part3.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 15:35:31 WARN Instrumentation: [5ce40a69] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/12/16 15:35:31 WARN Instrumentation: [7a4711e9] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/12/16 15:35:32 WARN Instrumentation: [36d2dd83] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Job_Worker2] Finished. Acc: 0.9006 | Time: 4.99s\n",
      "[Job_Master] Finished. Acc: 0.8966 | Time: 5.44s\n",
      "[Job_Worker1] Finished. Acc: 0.8977 | Time: 5.52s\n",
      "============================================================\n",
      "All Simultaneous Jobs Finished in: 5.52s\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the 3 independent jobs\n",
    "threads = []\n",
    "file_parts = [\n",
    "    (\"Job_Master\", \"/data/part1.csv\"),\n",
    "    (\"Job_Worker1\", \"/data/part2.csv\"),\n",
    "    (\"Job_Worker2\", \"/data/part3.csv\"),\n",
    "]\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "# launch threads to run simultaneously\n",
    "for name, path in file_parts:\n",
    "    t = threading.Thread(target=train_local_simulation, args=(name, path))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "# wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "global_end = time.time()\n",
    "total_simultaneous_time = global_end - global_start\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"All Simultaneous Jobs Finished in: {total_simultaneous_time:.2f}s\")\n",
    "print(\"_\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL COMPARISON: INDEPENDENT vs. DISTRIBUTED ===\n",
      "Metric               | Independent (Scenario A)  | Distributed (Scenario B) \n",
      "---------------------------------------------------------------------------\n",
      "Accuracy             | 0.8983                    | 0.9233                   \n",
      "Training Time        | 5.52                     s | 6.44                     s\n",
      "Data Scope           | Local Partition Only      | Global Unified Dataset   \n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comparative Analysis\n",
    "\n",
    "# calculate independent averages\n",
    "avg_ind_acc = sum(r[\"Accuracy\"] for r in independent_results.values()) / 3\n",
    "max_ind_time = max(r[\"Time\"] for r in independent_results.values())\n",
    "\n",
    "\n",
    "dist_acc = 0.9233\n",
    "dist_time = 6.44\n",
    "\n",
    "print(\"\\n=== FINAL COMPARISON: INDEPENDENT vs. DISTRIBUTED ===\")\n",
    "print(\n",
    "    f\"{'Metric':<20} | {'Independent (Scenario A)':<25} | {'Distributed (Scenario B)':<25}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Accuracy':<20} | {avg_ind_acc:<25.4f} | {dist_acc:<25.4f}\")\n",
    "print(f\"{'Training Time':<20} | {max_ind_time:<25.2f}s | {dist_time:<25.2f}s\")\n",
    "print(\n",
    "    f\"{'Data Scope':<20} | {'Local Partition Only':<25} | {'Global Unified Dataset':<25}\"\n",
    ")\n",
    "print(\"=\" * 75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
