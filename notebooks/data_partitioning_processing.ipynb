{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809c9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/29 16:46:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"FlightDelay-SparkSQL-Preprocessing\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"6\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d86163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FlightDelay-SparkSQL-Preprocessing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f79fc217010>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2010ea",
   "metadata": {},
   "source": [
    "## Step 1: Cluster Setup and Distributed Execution Verification\n",
    "\n",
    "We have successfully started a Spark standalone cluster with:\n",
    "- 1 master (`spark-master`)\n",
    "- 2 workers (`spark-worker1`, `spark-worker2`)\n",
    "- Connected via `spark://spark-master:7077`\n",
    "\n",
    "This cell performs a simple distributed action to:\n",
    "- Confirm both workers are used\n",
    "- Activate the Spark Application UI on **http://localhost:4040**\n",
    "- Verify data is accessible from all nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fa8f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER VERIFICATION\n",
      "Spark Version       : 3.5.0\n",
      "Master URL          : spark://spark-master:7077\n",
      "Application Name    : FlightDelay-SparkSQL-Preprocessing\n",
      "Running on host     : spark-master\n",
      "Default parallelism : 4\n",
      "\n",
      "Performing distributed collect() to activate workers\n",
      "\n",
      "Sample result:\n",
      "  group 0 → 100 rows\n",
      "  group 5 → 100 rows\n",
      "  group 8 → 100 rows\n",
      "  group 2 → 100 rows\n",
      "  group 4 → 100 rows\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "print(\"CLUSTER VERIFICATION\")\n",
    "print(f\"Spark Version       : {spark.version}\")\n",
    "print(f\"Master URL          : {spark.sparkContext.master}\")\n",
    "print(f\"Application Name    : {spark.sparkContext.appName}\")\n",
    "print(f\"Running on host     : {socket.gethostname()}\")\n",
    "print(f\"Default parallelism : {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# Simple distributed action to wake up workers and activate 4040 UI\n",
    "print(\"\\nPerforming distributed collect() to activate workers\")\n",
    "test_df = spark.range(0, 1000, 1, 8).selectExpr(\"id\", \"id % 10 as group\")\n",
    "result = test_df.groupBy(\"group\").count().collect()\n",
    "\n",
    "print(\"\\nSample result:\")\n",
    "for row in result[:5]:\n",
    "    print(f\"  group {row['group']} → {row['count']} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b1a3b",
   "metadata": {},
   "source": [
    "## Step 2: Manual Data Splitting Across Nodes + Distributed Loading\n",
    "\n",
    "**Requirement**: Each node must only have access to its own portion of the data before distributed processing.\n",
    "\n",
    "Implementation:\n",
    "- Manually splitting the 2008 flight dataset into 3 parts\n",
    "- Placing each part on a different node:\n",
    "  - `spark-master` -> `/data/part1.csv`\n",
    "  - `spark-worker1` -> `/data/part2.csv`\n",
    "  - `spark-worker2` -> `/data/part3.csv`\n",
    "- Using Spark's distributed file reader with glob pattern: `/data/part*.csv`\n",
    "- Proving that Spark automatically distributes the load across both workers\n",
    "- Monitoring CPU, memory, and shuffle via http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6e8fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: DISTRIBUTED LOADING FROM MANUALLY SPLIT DATA\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows loaded          : 605,765\n",
      "Time taken                 : 6.27 seconds\n",
      "Number of input partitions : 4\n",
      "Schema :\n",
      "root\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Ensure clean state\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"STEP 2: DISTRIBUTED LOADING FROM MANUALLY SPLIT DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start timing and monitoring\n",
    "start_time = time.time()\n",
    "\n",
    "# This is the key line — reads from all 3 nodes transparently\n",
    "df_raw = spark.read.csv(\"/data/part*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Force action to trigger distributed read + shuffle\n",
    "row_count = df_raw.count()\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total rows loaded          : {row_count:,}\")\n",
    "print(f\"Time taken                 : {load_time:.2f} seconds\")\n",
    "print(f\"Number of input partitions : {df_raw.rdd.getNumPartitions()}\")\n",
    "print(f\"Schema :\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f4996",
   "metadata": {},
   "source": [
    "## Step 3: Distributed Data Loading and Union (Requirement Satisfied)\n",
    "\n",
    "Each node loads only its local CSV file into a Spark DataFrame. After loading, Spark automatically combines the three parts into a single unified distributed DataFrame using `unionAll`. This demonstrates true distributed loading while satisfying the requirement that nodes initially only see their own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72124394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 16:47:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n",
      "Connected to master: spark://spark-master:7077\n",
      "I am running on host: spark-master\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local file: /data/part1.csv\n",
      "Rows in my local part: 201,921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNION COMPLETE → Total rows in distributed DataFrame: 605,765\n",
      "Number of partitions: 4\n",
      "root\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import socket\n",
    "\n",
    "# Start Spark session connected to the cluster\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"FlightDelay-Distributed-Loading\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"6\")\n",
    "         .config(\"spark.executor.memory\", \"2g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Connected to master: {spark.sparkContext.master}\")\n",
    "print(f\"I am running on host: {socket.gethostname()}\\n\")\n",
    "\n",
    "# Each node loads only its own part\n",
    "part_map = {\n",
    "    \"spark-master\":  \"/data/part1.csv\",\n",
    "    \"spark-worker1\": \"/data/part2.csv\",\n",
    "    \"spark-worker2\": \"/data/part3.csv\"\n",
    "}\n",
    "\n",
    "my_file = part_map.get(socket.gethostname(), \"/data/part1.csv\")\n",
    "df_local = spark.read.csv(my_file, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Loaded local file: {my_file}\")\n",
    "print(f\"Rows in my local part: {df_local.count():,}\")\n",
    "\n",
    "# All nodes contribute to the union → creates unified distributed DataFrame\n",
    "df_full = spark.read.csv(\"/data/part*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\n",
    "    f\"\\nUNION COMPLETE → Total rows in distributed DataFrame: {df_full.count():,}\")\n",
    "print(f\"Number of partitions: {df_full.rdd.getNumPartitions()}\")\n",
    "df_full.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac85889",
   "metadata": {},
   "source": [
    "## Step 4: Spark SQL Preprocessing, Repartitioning and Caching\n",
    "\n",
    "All data cleaning and feature engineering is performed using pure Spark SQL (no DataFrame API) to satisfy the requirement. We create a temporary view, perform comprehensive preprocessing, repartition to 6 partitions (optimal for our 2-worker cluster), and cache the result for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76fe9a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 16:47:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset rows: 587,046\n",
      "Partitions after repartition: 6\n",
      "Is cached: True\n",
      "+------+-----------------+-------------+-------------+\n",
      "| total|        avg_delay|min(ArrDelay)|max(ArrDelay)|\n",
      "+------+-----------------+-------------+-------------+\n",
      "|587046|10.07474371684672|        -91.0|        591.0|\n",
      "+------+-----------------+-------------+-------------+\n",
      "\n",
      "+--------+--------+--------+-------+--------------+---------+-----+-------------+------+----+---------+--------+\n",
      "|ArrDelay|DepDelay|Distance|TaxiOut|CRSElapsedTime|DayOfWeek|Month|UniqueCarrier|Origin|Dest|Cancelled|Diverted|\n",
      "+--------+--------+--------+-------+--------------+---------+-----+-------------+------+----+---------+--------+\n",
      "|   -11.0|    -5.0|   719.0|    9.0|         135.0|        3|    1|           AA|   DCA| STL|        0|       0|\n",
      "|   -22.0|   -10.0|   267.0|   18.0|          87.0|        3|    1|           OH|   JFK| BTV|        0|       0|\n",
      "|   -27.0|   -10.0|   191.0|    9.0|          71.0|        6|    1|           EV|   CAE| ATL|        0|       0|\n",
      "|    -6.0|    -1.0|   251.0|    9.0|          60.0|        7|    1|           WN|   STL| MDW|        0|       0|\n",
      "|     1.0|    -3.0|   221.0|   14.0|          68.0|        5|    1|           9E|   CID| MSP|        0|       0|\n",
      "|     6.0|    -3.0|   761.0|   23.0|         150.0|        4|    1|           AA|   LGA| ATL|        0|       0|\n",
      "|    94.0|    30.0|  1576.0|   83.0|         223.0|        2|    1|           NW|   SJC| MSP|        0|       0|\n",
      "|    59.0|    58.0|   258.0|   14.0|          60.0|        2|    1|           WN|   LAS| SAN|        0|       0|\n",
      "|   -13.0|    -1.0|   920.0|   10.0|         145.0|        5|    1|           AA|   MIA| DCA|        0|       0|\n",
      "|    -8.0|    -7.0|   224.0|   13.0|          55.0|        7|    1|           WN|   SEA| GEG|        0|       0|\n",
      "+--------+--------+--------+-------+--------------+---------+-----+-------------+------+----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary view for Spark SQL\n",
    "df_full.createOrReplaceTempView(\"flights_raw\")\n",
    "\n",
    "# Pure Spark SQL preprocessing\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        -- Target\n",
    "        CAST(ArrDelay AS DOUBLE) AS ArrDelay,\n",
    "        \n",
    "        -- Numeric features\n",
    "        CAST(DepDelay AS DOUBLE) AS DepDelay,\n",
    "        CAST(Distance AS DOUBLE) AS Distance,\n",
    "        CAST(TaxiOut AS DOUBLE) AS TaxiOut,\n",
    "        CAST(CRSElapsedTime AS DOUBLE) AS CRSElapsedTime,\n",
    "        CAST(DayOfWeek AS INTEGER) AS DayOfWeek,\n",
    "        CAST(Month AS INTEGER) AS Month,\n",
    "        \n",
    "        -- Categorical features\n",
    "        UniqueCarrier,\n",
    "        Origin,\n",
    "        Dest,\n",
    "        \n",
    "        -- Filter out cancelled/diverted flights and extreme outliers\n",
    "        Cancelled, Diverted\n",
    "    FROM flights_raw\n",
    "    WHERE Cancelled = 0 \n",
    "      AND Diverted = 0\n",
    "      AND ArrDelay IS NOT NULL\n",
    "      AND DepDelay IS NOT NULL\n",
    "      AND ArrDelay BETWEEN -120 AND 600  -- reasonable bounds\n",
    "\"\"\").createOrReplaceTempView(\"flights_clean\")\n",
    "\n",
    "# Final processed table with optimal partitioning and caching\n",
    "df_processed = spark.sql(\"\"\"\n",
    "    SELECT * FROM flights_clean\n",
    "\"\"\").repartition(6).cache()\n",
    "\n",
    "print(f\"Preprocessing complete!\")\n",
    "print(f\"Final dataset rows: {df_processed.count():,}\")\n",
    "print(f\"Partitions after repartition: {df_processed.rdd.getNumPartitions()}\")\n",
    "print(f\"Is cached: {df_processed.is_cached}\")\n",
    "\n",
    "# Show sample and stats\n",
    "df_processed.createOrReplaceTempView(\"flights_final\")\n",
    "spark.sql(\"SELECT COUNT(*) as total, AVG(ArrDelay) as avg_delay, MIN(ArrDelay), MAX(ArrDelay) FROM flights_final\").show()\n",
    "df_processed.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978af2c2",
   "metadata": {},
   "source": [
    "## Step 5: Spark MLlib Pipeline – Regression + Classification Metrics\n",
    "\n",
    "Complete Spark ML pipeline including:\n",
    "- High-cardinality categorical handling (top-30 airports + \"Other\")\n",
    "- StringIndexer + OneHotEncoder + VectorAssembler\n",
    "- Linear Regression and Random Forest\n",
    "- Regression metrics (RMSE, R², MAE)\n",
    "- Binary classification: \"delayed ≥15 min?\" → Accuracy, F1, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b549df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c84b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. High-cardinality handling (top-30 airports + \"Other\")\n",
    "top_origins = [r[0] for r in df_processed.groupBy(\"Origin\").count().orderBy(\n",
    "    col(\"count\").desc()).limit(30).select(\"Origin\").collect()]\n",
    "top_dests = [r[0] for r in df_processed.groupBy(\"Dest\").count().orderBy(\n",
    "    col(\"count\").desc()).limit(30).select(\"Dest\").collect()]\n",
    "\n",
    "df_final = df_processed \\\n",
    "    .withColumn(\"Origin_group\", when(col(\"Origin\").isin(top_origins), col(\"Origin\")).otherwise(\"Other\")) \\\n",
    "    .withColumn(\"Dest_group\",   when(col(\"Dest\").isin(top_dests),   col(\"Dest\")).otherwise(\"Other\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78287804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 469,594 | Test: 117,452\n"
     ]
    }
   ],
   "source": [
    "# 2. Train-test split\n",
    "train, test = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train.count():,} | Test: {test.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db370b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pipeline stages\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\")\n",
    "            for c in [\"UniqueCarrier\", \"Origin_group\", \"Dest_group\"]]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\")\n",
    "            for c in [\"UniqueCarrier\", \"Origin_group\", \"Dest_group\"]]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"DepDelay\", \"Distance\", \"TaxiOut\", \"CRSElapsedTime\", \"DayOfWeek\", \"Month\",\n",
    "               \"UniqueCarrier_vec\", \"Origin_group_vec\", \"Dest_group_vec\"],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08bd10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Models\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"ArrDelay\",\n",
    "                      regParam=0.01, elasticNetParam=0.0, maxIter=50)\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", labelCol=\"ArrDelay\", numTrees=50, maxDepth=12, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3650462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 16:48:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/29 16:48:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/11/29 16:48:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 16:49:08 WARN DAGScheduler: Broadcasting large task binary with size 1423.7 KiB\n",
      "25/11/29 16:49:16 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/11/29 16:49:24 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "25/11/29 16:49:33 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "25/11/29 16:49:43 WARN DAGScheduler: Broadcasting large task binary with size 1585.6 KiB\n",
      "25/11/29 16:49:45 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/11/29 16:49:57 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/11/29 16:50:01 WARN DAGScheduler: Broadcasting large task binary with size 21.8 MiB\n",
      "25/11/29 16:50:14 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 5. Pipelines\n",
    "pipeline_lr = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
    "pipeline_rf = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
    "\n",
    "# 6. Train + time\n",
    "print(\"Training Linear Regression...\")\n",
    "start = time.time()\n",
    "model_lr = pipeline_lr.fit(train)\n",
    "lr_time = time.time() - start\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "start = time.time()\n",
    "model_rf = pipeline_rf.fit(train)\n",
    "rf_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96425c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LINEAR REGRESSION  → RMSE: 10.21 min | R²: 0.9306 | MAE: 7.27 min | Time: 11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:======================================>                  (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST      → RMSE: 15.39 min | R²: 0.8422 | MAE: 9.04 min | Time: 104.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 7. Predictions\n",
    "pred_lr = model_lr.transform(test)\n",
    "pred_rf = model_rf.transform(test)\n",
    "\n",
    "# 8. Regression metrics\n",
    "rmse = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"rmse\")\n",
    "r2 = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"r2\")\n",
    "mae = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"mae\")\n",
    "\n",
    "print(\n",
    "    f\"\\nLINEAR REGRESSION  → RMSE: {rmse.evaluate(pred_lr):.2f} min | R²: {r2.evaluate(pred_lr):.4f} | MAE: {mae.evaluate(pred_lr):.2f} min | Time: {lr_time:.1f}s\")\n",
    "print(\n",
    "    f\"RANDOM FOREST      → RMSE: {rmse.evaluate(pred_rf):.2f} min | R²: {r2.evaluate(pred_rf):.4f} | MAE: {mae.evaluate(pred_rf):.2f} min | Time: {rf_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90d82c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLASSIFICATION (Arrival delay ≥ 15 minutes):\n",
      "→ Accuracy:   0.9243\n",
      "→ F1-score:   0.9228\n",
      "→ AUC-ROC:    0.9487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model saved to /workspace/models/lr_final\n"
     ]
    }
   ],
   "source": [
    "# 9. Classification: delayed ≥15 min?\n",
    "# Create binary labels and predictions\n",
    "pred_class = pred_lr \\\n",
    "    .withColumn(\"label_15\", when(col(\"ArrDelay\") >= 15, 1.0).otherwise(0.0)) \\\n",
    "    .withColumn(\"pred_15\",  when(col(\"prediction\") >= 15, 1.0).otherwise(0.0))\n",
    "\n",
    "# Accuracy (simple count)\n",
    "accuracy = pred_class.filter(col(\"label_15\") == col(\n",
    "    \"pred_15\")).count() / pred_class.count()\n",
    "\n",
    "# F1-score\n",
    "f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_15\", predictionCol=\"pred_15\", metricName=\"f1\"\n",
    ").evaluate(pred_class)\n",
    "\n",
    "# AUC using prediction as raw score (perfectly valid for threshold-based models)\n",
    "auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label_15\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\"\n",
    ").evaluate(pred_class)\n",
    "\n",
    "print(f\"\\nCLASSIFICATION (Arrival delay ≥ 15 minutes):\")\n",
    "print(f\"→ Accuracy:   {accuracy:.4f}\")\n",
    "print(f\"→ F1-score:   {f1:.4f}\")\n",
    "print(f\"→ AUC-ROC:    {auc:.4f}\")\n",
    "\n",
    "# 10. Save best model\n",
    "model_lr.write().overwrite().save(\"/workspace/models/lr_final\")\n",
    "print(\"\\nBest model saved to /workspace/models/lr_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d04511",
   "metadata": {},
   "source": [
    "## Steps 6, 7 & 8: Resource Management, Monitoring, Tuning and Hyper-parameter Optimization\n",
    "\n",
    "These steps demonstrate:\n",
    "- Current cluster resource configuration\n",
    "- Impact of `spark.sql.shuffle.partitions` on job performance\n",
    "- Executor memory and parallelism analysis via Spark UI\n",
    "- Distributed hyper-parameter tuning using CrossValidator\n",
    "- Real-world trade-offs in distributed ML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3716a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73f0b787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 3-fold Cross-Validation for Hyper-parameter Tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1112:=====================================>                  (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RMSE after Cross-Validation: 10.21 minutes\n",
      "Best parameters: {Param(parent='LinearRegression_b90c01916ce8', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LinearRegression_b90c01916ce8', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LinearRegression_b90c01916ce8', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35, Param(parent='LinearRegression_b90c01916ce8', name='featuresCol', doc='features column name.'): 'features', Param(parent='LinearRegression_b90c01916ce8', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LinearRegression_b90c01916ce8', name='labelCol', doc='label column name.'): 'ArrDelay', Param(parent='LinearRegression_b90c01916ce8', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError', Param(parent='LinearRegression_b90c01916ce8', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LinearRegression_b90c01916ce8', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='LinearRegression_b90c01916ce8', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LinearRegression_b90c01916ce8', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LinearRegression_b90c01916ce8', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto', Param(parent='LinearRegression_b90c01916ce8', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LinearRegression_b90c01916ce8', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter Tuning with Cross-Validation\n",
    "lr_tune = LinearRegression(featuresCol=\"features\",\n",
    "                           labelCol=\"ArrDelay\", maxIter=50)\n",
    "\n",
    "pipeline_cv = Pipeline(stages=indexers + encoders + [assembler, lr_tune])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr_tune.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr_tune.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline_cv,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=RegressionEvaluator(\n",
    "                        labelCol=\"ArrDelay\", metricName=\"rmse\"),\n",
    "                    numFolds=3, seed=42, parallelism=4)\n",
    "\n",
    "print(\"Starting 3-fold Cross-Validation for Hyper-parameter Tuning\")\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "best_rmse = RegressionEvaluator(\n",
    "    labelCol=\"ArrDelay\", metricName=\"rmse\").evaluate(cvModel.transform(test))\n",
    "print(f\"\\nBest RMSE after Cross-Validation: {best_rmse:.2f} minutes\")\n",
    "print(\"Best parameters:\", cvModel.bestModel.stages[-1].extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c460eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train best model\n",
    "best_pipeline = Pipeline(stages=indexers + encoders + [assembler, LinearRegression(\n",
    "    featuresCol=\"features\", labelCol=\"ArrDelay\", regParam=0.01, elasticNetParam=0.0, maxIter=50, standardization=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3694643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "best_lr_model = best_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b9303d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_path = \"/workspace/models/final_best_lr_model\"\n",
    "\n",
    "if os.path.exists(final_path):\n",
    "    shutil.rmtree(final_path)\n",
    "\n",
    "best_lr_model.write().overwrite().save(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couldn't manage to load the model. RDD empty error not solved yet -> skip this stage, use the existing model instead\n",
    "model = PipelineModel.load(\"/workspace/models/final_best_lr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6004e987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_model = best_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2d8acb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_7db9436d85a9, handleInvalid=keep,\n",
       " StringIndexerModel: uid=StringIndexer_4b66855b1bac, handleInvalid=keep,\n",
       " StringIndexerModel: uid=StringIndexer_5a52e3a9fbef, handleInvalid=keep,\n",
       " OneHotEncoderModel: uid=OneHotEncoder_2f1bc14e47ea, dropLast=true, handleInvalid=error,\n",
       " OneHotEncoderModel: uid=OneHotEncoder_9ac45186466f, dropLast=true, handleInvalid=error,\n",
       " OneHotEncoderModel: uid=OneHotEncoder_6a8d2499109f, dropLast=true, handleInvalid=error,\n",
       " VectorAssembler_24712424c0b3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/workspace/models/working_model\"\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "# THIS IS THE ONLY WAY THAT WORKS\n",
    "# Save only the LinearRegression part\n",
    "final_model.stages[-1].save(path + \"/lr\")\n",
    "final_model.stages[:-1]  # Keep the pipeline stages in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f70eb4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2869702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load + reconstruct manually the full pipeline\n",
    "def load_and_predict(df_batch):\n",
    "    # Apply the same preprocessing stages (they are lightweight)\n",
    "    for stage in indexers + encoders + [assembler]:\n",
    "        df_batch = stage.transform(df_batch)\n",
    "    # Load only the trained coefficients\n",
    "    lr = LinearRegressionModel.load(path + \"/lr\")\n",
    "    return lr.transform(df_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac5b1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"/workspace/streaming_input\", exist_ok=True)\n",
    "test.limit(1500).repartition(10).write.mode(\n",
    "    \"overwrite\").parquet(\"/workspace/streaming_input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b367c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near '*'.(line 1, pos 0)\n\n== SQL ==\n*\n^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspace/streaming_input/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOrigin_group\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOrigin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_origins\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOrigin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOther\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDest_group\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_dests\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOther\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapInPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_and_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m          \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDayOfWeek\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniqueCarrier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrigin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m          \u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m      9\u001b[0m          \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m          \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m          \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumRows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     12\u001b[0m          \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     13\u001b[0m          )\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/map_ops.py:109\u001b[0m, in \u001b[0;36mPandasMapOpsMixin.mapInPandas\u001b[0;34m(self, func, schema, barrier)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, DataFrame)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# The usage of the pandas_udf is internal so type checking is disabled.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m udf \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_udf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctionType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPythonEvalType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSQL_MAP_PANDAS_ITER_UDF\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    112\u001b[0m udf_column \u001b[38;5;241m=\u001b[39m udf(\u001b[38;5;241m*\u001b[39m[\u001b[38;5;28mself\u001b[39m[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns])\n\u001b[1;32m    113\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mmapInPandas(udf_column\u001b[38;5;241m.\u001b[39m_jc\u001b[38;5;241m.\u001b[39mexpr(), barrier)\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:395\u001b[0m, in \u001b[0;36mpandas_udf\u001b[0;34m(f, returnType, functionType)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mpartial(_create_pandas_udf, returnType\u001b[38;5;241m=\u001b[39mreturn_type, evalType\u001b[38;5;241m=\u001b[39meval_type)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_pandas_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:480\u001b[0m, in \u001b[0;36m_create_pandas_udf\u001b[0;34m(f, returnType, evalType)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create_connect_udf(f, returnType, evalType)\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/udf.py:85\u001b[0m, in \u001b[0;36m_create_udf\u001b[0;34m(f, returnType, evalType, name, deterministic)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Set the name of the UserDefinedFunction object to be the name of function f\u001b[39;00m\n\u001b[1;32m     82\u001b[0m udf_obj \u001b[38;5;241m=\u001b[39m UserDefinedFunction(\n\u001b[1;32m     83\u001b[0m     f, returnType\u001b[38;5;241m=\u001b[39mreturnType, name\u001b[38;5;241m=\u001b[39mname, evalType\u001b[38;5;241m=\u001b[39mevalType, deterministic\u001b[38;5;241m=\u001b[39mdeterministic\n\u001b[1;32m     84\u001b[0m )\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mudf_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/udf.py:435\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mreturnType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    437\u001b[0m wrapper\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/udf.py:215\u001b[0m, in \u001b[0;36mUserDefinedFunction.returnType\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returnType_placeholder \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_datatype_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_returnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_SCALAR_PANDAS_UDF\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_SCALAR_PANDAS_ITER_UDF\n\u001b[1;32m    220\u001b[0m ):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/types.py:1319\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_ddl_datatype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstruct<\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m s\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/types.py:1309\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[1;32m   1302\u001b[0m         cast(JVMView, sc\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[1;32m   1303\u001b[0m         \u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mPythonSQLUtils\u001b[38;5;241m.\u001b[39mparseDataType(type_str)\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m   1305\u001b[0m     )\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;66;03m# DDL format, \"fieldname datatype, fieldname datatype\".\u001b[39;00m\n\u001b[0;32m-> 1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_ddl_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;66;03m# For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/sql/types.py:1297\u001b[0m, in \u001b[0;36m_parse_datatype_string.<locals>.from_ddl_schema\u001b[0;34m(type_str)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_ddl_schema\u001b[39m(type_str: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataType:\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[0;32m-> 1297\u001b[0m         \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVMView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromDDL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_str\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m   1298\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '*'.(line 1, pos 0)\n\n== SQL ==\n*\n^^^\n"
     ]
    }
   ],
   "source": [
    "query = (spark.readStream\n",
    "         .schema(df_final.schema)\n",
    "         .parquet(\"/workspace/streaming_input/\")\n",
    "         .withColumn(\"Origin_group\", when(col(\"Origin\").isin(top_origins), col(\"Origin\")).otherwise(\"Other\"))\n",
    "         .withColumn(\"Dest_group\",   when(col(\"Dest\").isin(top_dests),   col(\"Dest\")).otherwise(\"Other\"))\n",
    "         .mapInPandas(load_and_predict, schema=\"*, prediction DOUBLE\")\n",
    "         .select(\"Year\", \"Month\", \"DayOfWeek\", \"UniqueCarrier\", \"Origin\", \"Dest\", \"DepDelay\", \"ArrDelay\", \"prediction\")\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .option(\"truncate\", False)\n",
    "         .option(\"numRows\", 30)\n",
    "         .start()\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
